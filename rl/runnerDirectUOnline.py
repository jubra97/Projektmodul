import os
import pathlib
import json

import numpy as np
import torch as th
from stable_baselines3 import DDPG
from stable_baselines3.common.callbacks import CallbackList
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.noise import NormalActionNoise

from rl.CustomEvalCallback import CustomEvalCallback
from rl.ActionNoiseCallback import ActionNoiseCallback
from rl.envs.DirectControllerOnline import DirectControllerOnline
from rl.envs.DirectControllerOnlineConnection import DirectControllerOnlineConnection
from rl import utils, custom_policy

# set your observation options, depending on the function different other keywords can be used:

# If function is "raw_with_vel", "error_with_vel" or "error_with_extra_components" you can use the "average_length"
# keyword to define of how many last sensor data points an average value should be used

# If the function is "raw_with_last_states" or "error_with_last_states" the "history_length" keyword defines how many
# last time steps should be included in the observation

# If the function is "raw_with_last_states" with the "use_u" keyword you can choose if you want to use w and y or
# w, u and y as observation

# If the function is "error_with_extra_components" you can use an "obs_config" dict to add different options:
# if no dict is given only the error is used as observation. The dict hast the following options:
# 'd', 'i', 'input', 'output', 'input_ve', 'output_vel'. For every keyword that is true the corresponding value is added
# to the observation
observation_options = {
    # other possible options: "raw_with_vel", "raw_with_last_states",
    # "error_with_vel", "error_with_last_states", "error_with_extra_components"
    "function": "error_with_vel",
    "average_length": 1,
}

# for more information look at the docstring of DirectControl.create_reward()
# The Reward function is customizable with the different keywords:
# discrete_bonus: Add a discrete bonus if e is small, helps to achieve more precise final value
# error_pen_fun: Use a function to calculate the penalty generated by e. E.g np.sqrt, or np.square
# oscillation_pen_fun: Use a function to calculate the penalty generated by input change. E.g np.sqrt, or np.square
# oscillation_pen_gain: Weight between error_pen (1) and input_oscillation_pen (weight)
# oscillation_pen_dependent_on_error: weight input_oscillation_pen with 1/e; by this oscillation is higher
# penalized if the error is small
reward_options = {
    "function": "normal",  # add your own reward function if you want to
    "discrete_bonus": True,
    "oscillation_pen_dependent_on_error": False,
    "oscillation_pen_fun": np.sqrt,
    "oscillation_pen_gain": 0.1,
    "error_pen_fun": None,
}

env_options = {
    "online_sys": DirectControllerOnlineConnection(),  # add the online connection
    "sensor_freq": 4_000,  # generate sensor data with 4_000 Hz
    "output_freq": 100,  # update output with 100 Hz
    "observation_kwargs": observation_options,
    "reward_kwargs": reward_options,
    "log": False,  # log false for training envs
}

policy_options = {
    "actor_layers": 2,
    "actor_layer_width": 10,  # amount of neurons per layer in the hidden layers
    "actor_activation_fun": th.nn.ReLU(),
    "actor_end_activation_fun": th.nn.Tanh(),  # must be a activation function that clips the value between (-1, 1)
    "actor_bias": False,
    "critic_layers": 2,
    "critic_layer_width": 200,  # amount of neurons per layer in the hidden layers
    "critic_activation_fun": th.nn.Tanh(),
    "critic_bias": True,
}


# save_path: Where is the final eval and the final model saved
# tensorboard_log_name: there is the tensorboard file saved
# action_noise: use tuple with three entries (start, end, steps) for linear action noise and scalar for
# constant action noise, use None if no action noise is needed
rl_options = {
    "save_path": "controller_test_online2",
    "tensorboard_log_name": "tensorboard_controller_test_online2",
    "timesteps": 100_000,
    "action_noise": 0.05,
}

params_dict = {"env_options": env_options,
               "policy_options": policy_options,
               "rl_options": rl_options}


if __name__ == "__main__":
    # check for existing entries and add a new one
    pathlib.Path(rl_options["save_path"]).mkdir(exist_ok=True)
    dir_entries = os.listdir(rl_options["save_path"])
    if dir_entries:
        dir_entries_int = [int(d) for d in dir_entries]
        run_nbr = max(dir_entries_int) + 1
    else:
        run_nbr = 0

    # create multiple envs for parallel use
    env = DirectControllerOnline(**env_options)

    env_options["log"] = True  # log only true for evaluation envs

    # create callback that logs matplotlib figures on tensorboard and saves possible best model
    online_eval_env = DirectControllerOnline(**env_options)  # create eval env
    online_eval_env = Monitor(online_eval_env)
    eval_callback = CustomEvalCallback(online_eval_env,
                                       eval_freq=1500,
                                       deterministic=True,
                                       best_model_save_path=f"{rl_options['save_path']}\\{run_nbr}\\best_model")

    callbacks = CallbackList([eval_callback])

    # add action noise; add action noise callback if needed
    if isinstance(rl_options["action_noise"], tuple) and len(rl_options["action_noise"]) == 3:
        action_noise_callback = ActionNoiseCallback(rl_options["action_noise"][0], rl_options["action_noise"][1], rl_options["action_noise"][2])
        callbacks.callbacks.append(action_noise_callback)
        action_noise = None
    elif rl_options["action_noise"]:
        # create normal action noise
        n_actions = env.action_space.shape[-1]
        action_noise = NormalActionNoise(mean=np.zeros(n_actions),
                                         sigma=float(rl_options["action_noise"]) * np.ones(n_actions))
    else:
        action_noise = None

    # # use DDPG and create a tensorboard
    # # start tensorboard server with tensorboard --logdir ./{tensorboard_log}/
    model = DDPG(custom_policy.CustomDDPGPolicy,
                 env,
                 learning_starts=6000,  # delay start to compensate for bad starting conditions
                 verbose=2,
                 action_noise=action_noise,
                 tensorboard_log=rl_options["tensorboard_log_name"],
                 policy_kwargs=policy_options,
                 )

    model.learn(total_timesteps=rl_options["timesteps"], tb_log_name=f"{run_nbr}", callback=callbacks)
    # #
    # # # save model if you want to
    model.save(f"{rl_options['save_path']}\\{run_nbr}\\model")

    with open(f"{rl_options['save_path']}\\{run_nbr}\\extra_info.json", 'w+') as f:
        json.dump(params_dict, f, indent=4, default=utils.custom_default_json)

    # export onnx model if you want to run it directly in the plc
    utils.export_onnx(model, f"{rl_options['save_path']}\\{run_nbr}\\model.onnx")


